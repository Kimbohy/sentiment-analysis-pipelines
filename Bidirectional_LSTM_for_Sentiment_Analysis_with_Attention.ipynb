{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a3d3d3",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM for Sentiment Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook implements a **Bidirectional LSTM** for multiclass sentiment analysis (Negative, Neutral, Positive).\n",
    "\n",
    "### Key Features:\n",
    "- **Word2Vec embeddings** (frozen) for semantic representations\n",
    "- **Bidirectional LSTM** to capture context from both directions\n",
    "- **Attention mechanism** to focus on important tokens\n",
    "- **Regularization techniques**: dropout, word dropout, weight decay\n",
    "- **Class weights** to handle imbalanced data\n",
    "- **Full 9-epoch training** with detailed performance analysis\n",
    "\n",
    "### Model Configuration:\n",
    "- Embedding dim: 128\n",
    "- Hidden dim: 96\n",
    "- Dropout: 0.4\n",
    "- Word dropout: 0.05\n",
    "- Weight decay: 3e-3\n",
    "- Learning rate: 0.001 with ReduceLROnPlateau scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204452c",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163cfb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.532043Z",
     "iopub.status.busy": "2026-02-06T20:10:19.531367Z",
     "iopub.status.idle": "2026-02-06T20:10:19.537966Z",
     "shell.execute_reply": "2026-02-06T20:10:19.537237Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.531999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91ab4b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.539542Z",
     "iopub.status.busy": "2026-02-06T20:10:19.539271Z",
     "iopub.status.idle": "2026-02-06T20:10:19.552027Z",
     "shell.execute_reply": "2026-02-06T20:10:19.551417Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.539523Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cb315",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "689fc89e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.553001Z",
     "iopub.status.busy": "2026-02-06T20:10:19.552759Z",
     "iopub.status.idle": "2026-02-06T20:10:19.564902Z",
     "shell.execute_reply": "2026-02-06T20:10:19.564182Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.552977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Model — balanced capacity\n",
    "    EMBEDDING_DIM = 128\n",
    "    HIDDEN_DIM = 96          # middle ground: 64 was too small, 128 too large\n",
    "    NUM_LAYERS = 1\n",
    "    DROPOUT = 0.4\n",
    "    WORD_DROPOUT = 0.05      # less aggressive: 0.1 was too much\n",
    "    NUM_CLASSES = 3\n",
    "    min_alpha=0.0001\n",
    "\n",
    "    # Training\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 9\n",
    "    MAX_SEQ_LENGTH = 64\n",
    "    VOCAB_SIZE = 15000\n",
    "\n",
    "    # Word2Vec\n",
    "    W2V_SIZE = 128\n",
    "    W2V_WINDOW = 5\n",
    "    W2V_MIN_COUNT = 1\n",
    "    W2V_WORKERS = 4\n",
    "\n",
    "    # Regularization\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    WEIGHT_DECAY = 3e-3      # balanced: strong enough to prevent overfitting\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64418dc",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a132cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.619252Z",
     "iopub.status.busy": "2026-02-06T20:10:19.618679Z",
     "iopub.status.idle": "2026-02-06T20:10:19.627583Z",
     "shell.execute_reply": "2026-02-06T20:10:19.626893Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.619232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Advanced text preprocessing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)                                     # Remove email addresses\n",
    "    text = re.sub(r'@\\w+', '', text)                                        # Remove mentions\n",
    "    text = re.sub(r'#', '', text)                                           # Remove hashtags but keep the text\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)                                 # Remove punctuation and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                                # Remove extra whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text, remove_stopwords=True):\n",
    "    \"\"\"Tokenize and optionally remove stopwords\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        keep_words = {'not', 'no', 'nor', 'but', 'however', 'yet', 'very', 'too', \n",
    "                     'never', 'nothing', 'neither', 'nobody', 'nowhere'}\n",
    "        stop_words = stop_words - keep_words\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def build_vocab(texts, max_vocab_size):\n",
    "    \"\"\"Build vocabulary from texts\"\"\"\n",
    "    word_freq = Counter()\n",
    "    for text in texts:\n",
    "        word_freq.update(text)\n",
    "    \n",
    "    most_common = word_freq.most_common(max_vocab_size - 2)\n",
    "    \n",
    "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    word2idx.update({word: idx + 2 for idx, (word, _) in enumerate(most_common)})\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "def text_to_sequence(tokens, word2idx, max_length):\n",
    "    \"\"\"Convert tokens to padded sequence of indices\"\"\"\n",
    "    sequence = [word2idx.get(word, word2idx['<UNK>']) for word in tokens]\n",
    "    \n",
    "    if len(sequence) < max_length:\n",
    "        sequence = sequence + [word2idx['<PAD>']] * (max_length - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_length]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "print(\"Text preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166678b",
   "metadata": {},
   "source": [
    "## 4. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a3c0875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.798118Z",
     "iopub.status.busy": "2026-02-06T20:10:19.797827Z",
     "iopub.status.idle": "2026-02-06T20:10:19.802644Z",
     "shell.execute_reply": "2026-02-06T20:10:19.801899Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.798086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe33e92",
   "metadata": {},
   "source": [
    "## 5. Model Architecture\n",
    "\n",
    "### Components:\n",
    "1. **Embedding Layer**: Frozen Word2Vec embeddings (128-dim)\n",
    "2. **Bidirectional LSTM**: 96 hidden units, captures context from both directions\n",
    "3. **Attention Layer**: Learns to focus on important tokens in the sequence\n",
    "4. **Classifier**: Two-layer feedforward network with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ad113319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.804253Z",
     "iopub.status.busy": "2026-02-06T20:10:19.804026Z",
     "iopub.status.idle": "2026-02-06T20:10:19.817287Z",
     "shell.execute_reply": "2026-02-06T20:10:19.816603Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.804235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Simple additive attention over LSTM outputs.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        context = torch.sum(weights * lstm_output, dim=1)\n",
    "        return context\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM + Attention — kept deliberately small.\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,\n",
    "                 num_classes, dropout, pretrained_embeddings=None,\n",
    "                 word_dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.word_dropout = word_dropout\n",
    "\n",
    "        # Embeddings are frozen — W2V coverage is ~100%, no need to fine-tune\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False  # permanently frozen\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        self.attention = AttentionLayer(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Word dropout: randomly replace tokens with <UNK> (idx 1) during training\n",
    "        if self.training and self.word_dropout > 0:\n",
    "            mask = torch.bernoulli(\n",
    "                torch.full_like(x, 1.0 - self.word_dropout, dtype=torch.float)\n",
    "            ).long()\n",
    "            # Keep padding (idx 0) untouched\n",
    "            pad_mask = (x != 0).long()\n",
    "            x = x * mask + pad_mask * (1 - mask)  # replace with 1 (<UNK>)\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        context = self.attention(lstm_out)\n",
    "        return self.fc(self.dropout(context))\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31af60",
   "metadata": {},
   "source": [
    "## 6. Word2Vec and Embedding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1bef2ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.818468Z",
     "iopub.status.busy": "2026-02-06T20:10:19.818154Z",
     "iopub.status.idle": "2026-02-06T20:10:19.832747Z",
     "shell.execute_reply": "2026-02-06T20:10:19.832080Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.818440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec and embedding functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_word2vec(tokenized_texts, config):\n",
    "    \"\"\"Train Word2Vec embeddings with better parameters\"\"\"\n",
    "    print(\"Training Word2Vec embeddings...\")\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=tokenized_texts,\n",
    "        vector_size=config.W2V_SIZE,\n",
    "        window=config.W2V_WINDOW,\n",
    "        min_count=config.W2V_MIN_COUNT,\n",
    "        workers=config.W2V_WORKERS,\n",
    "        sg=1,\n",
    "        epochs=15,  # More epochs for better embeddings\n",
    "        negative=10,\n",
    "        alpha=0.025,\n",
    "        min_alpha=0.0001,\n",
    "        seed=SEED\n",
    "    )\n",
    "    return w2v_model\n",
    "\n",
    "def create_embedding_matrix(word2idx, w2v_model, embedding_dim):\n",
    "    \"\"\"Create embedding matrix from Word2Vec model\"\"\"\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "    \n",
    "    embedding_matrix[0] = np.zeros(embedding_dim)\n",
    "    \n",
    "    found = 0\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "            found += 1\n",
    "    \n",
    "    print(f\"Found {found}/{vocab_size} words in Word2Vec model ({found/vocab_size*100:.2f}%)\")\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute class weights for imbalanced data\"\"\"\n",
    "    class_counts = np.bincount(labels)\n",
    "    total = len(labels)\n",
    "    weights = total / (len(class_counts) * class_counts)\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "print(\"Word2Vec and embedding functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d92cf",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4f618b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.834636Z",
     "iopub.status.busy": "2026-02-06T20:10:19.834403Z",
     "iopub.status.idle": "2026-02-06T20:10:19.849461Z",
     "shell.execute_reply": "2026-02-06T20:10:19.848751Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.834617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_clip):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for sequences, labels in dataloader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in dataloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, predictions, true_labels\n",
    "\n",
    "print(\"Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f89fd5",
   "metadata": {},
   "source": [
    "## 8. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d54d6b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:19.850318Z",
     "iopub.status.busy": "2026-02-06T20:10:19.850117Z",
     "iopub.status.idle": "2026-02-06T20:10:21.593773Z",
     "shell.execute_reply": "2026-02-06T20:10:21.593174Z",
     "shell.execute_reply.started": "2026-02-06T20:10:19.850300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Bidirectional LSTM with Attention — Regularized\n",
      "================================================================================\n",
      "\n",
      " Loading dataset...\n",
      "Train: 31232  Val: 5205  Test: 5206\n",
      "\n",
      "Class distribution (train):\n",
      "label\n",
      "0     9105\n",
      "1    11649\n",
      "2    10478\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Bidirectional LSTM with Attention — Regularized\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\n Loading dataset...\")\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    val_df = pd.DataFrame(dataset['validation'])\n",
    "    test_df = pd.DataFrame(dataset['test'])\n",
    "except Exception as e:\n",
    "    print(f\"HuggingFace load failed ({e}), falling back to CSV...\")\n",
    "    train_df = pd.read_csv('train_df.csv')\n",
    "    val_df = pd.read_csv('val_df.csv')\n",
    "    test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)}  Val: {len(val_df)}  Test: {len(test_df)}\")\n",
    "print(\"\\nClass distribution (train):\")\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b410648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:21.594759Z",
     "iopub.status.busy": "2026-02-06T20:10:21.594516Z",
     "iopub.status.idle": "2026-02-06T20:10:29.538624Z",
     "shell.execute_reply": "2026-02-06T20:10:29.538036Z",
     "shell.execute_reply.started": "2026-02-06T20:10:21.594728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessing & tokenizing...\n",
      "Tokenization complete. Sample tokens: ['cooking', 'microwave', 'pizzas', 'yummy']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "print(\"\\n Preprocessing & tokenizing...\")\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "train_tokens = train_df['text_clean'].apply(tokenize_text).tolist()\n",
    "val_tokens = val_df['text_clean'].apply(tokenize_text).tolist()\n",
    "test_tokens = test_df['text_clean'].apply(tokenize_text).tolist()\n",
    "\n",
    "print(f\"Tokenization complete. Sample tokens: {train_tokens[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c2af7",
   "metadata": {},
   "source": [
    "## 9. Build Vocabulary and Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b45d8bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:29.539628Z",
     "iopub.status.busy": "2026-02-06T20:10:29.539402Z",
     "iopub.status.idle": "2026-02-06T20:10:53.318287Z",
     "shell.execute_reply": "2026-02-06T20:10:53.317661Z",
     "shell.execute_reply.started": "2026-02-06T20:10:29.539607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building vocabulary...\n",
      "Vocabulary size: 15000\n",
      "Training Word2Vec embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary (train only)\n",
    "print(\" Building vocabulary...\")\n",
    "word2idx, idx2word = build_vocab(train_tokens, config.VOCAB_SIZE)\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "# Train Word2Vec on ALL tokenized texts for better coverage\n",
    "all_tokens = train_tokens + val_tokens + test_tokens\n",
    "w2v_model = train_word2vec(all_tokens, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36178495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:53.319592Z",
     "iopub.status.busy": "2026-02-06T20:10:53.319223Z",
     "iopub.status.idle": "2026-02-06T20:10:53.403944Z",
     "shell.execute_reply": "2026-02-06T20:10:53.403357Z",
     "shell.execute_reply.started": "2026-02-06T20:10:53.319562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating embedding matrix...\n",
      "Found 14998/15000 words in Word2Vec model (99.99%)\n",
      "Embedding matrix shape: torch.Size([15000, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\" Creating embedding matrix...\")\n",
    "embedding_matrix = create_embedding_matrix(word2idx, w2v_model, config.EMBEDDING_DIM)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18f395",
   "metadata": {},
   "source": [
    "## 10. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42c33605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:53.405216Z",
     "iopub.status.busy": "2026-02-06T20:10:53.404874Z",
     "iopub.status.idle": "2026-02-06T20:10:53.755286Z",
     "shell.execute_reply": "2026-02-06T20:10:53.754700Z",
     "shell.execute_reply.started": "2026-02-06T20:10:53.405186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Converting to sequences...\n",
      "DataLoaders created: 488 train batches, 82 val batches\n"
     ]
    }
   ],
   "source": [
    "# Convert to sequences\n",
    "print(\" Converting to sequences...\")\n",
    "train_sequences = [text_to_sequence(t, word2idx, config.MAX_SEQ_LENGTH) for t in train_tokens]\n",
    "val_sequences = [text_to_sequence(t, word2idx, config.MAX_SEQ_LENGTH) for t in val_tokens]\n",
    "test_sequences = [text_to_sequence(t, word2idx, config.MAX_SEQ_LENGTH) for t in test_tokens]\n",
    "\n",
    "train_dataset = SentimentDataset(train_sequences, train_df['label'].values)\n",
    "val_dataset = SentimentDataset(val_sequences, val_df['label'].values)\n",
    "test_dataset = SentimentDataset(test_sequences, test_df['label'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"DataLoaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7a31e",
   "metadata": {},
   "source": [
    "## 11. Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "36704428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:53.757265Z",
     "iopub.status.busy": "2026-02-06T20:10:53.757035Z",
     "iopub.status.idle": "2026-02-06T20:10:53.782946Z",
     "shell.execute_reply": "2026-02-06T20:10:53.782402Z",
     "shell.execute_reply.started": "2026-02-06T20:10:53.757245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Initializing model...\n",
      "BiLSTMClassifier(\n",
      "  (embedding): Embedding(15000, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 96, batch_first=True, bidirectional=True)\n",
      "  (attention): AttentionLayer(\n",
      "    (attention): Linear(in_features=192, out_features=1, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=96, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=96, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 2,112,580  (trainable: 192,580)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(\"\\n Initializing model...\")\n",
    "model = BiLSTMClassifier(\n",
    "    vocab_size=len(word2idx),\n",
    "    embedding_dim=config.EMBEDDING_DIM,\n",
    "    hidden_dim=config.HIDDEN_DIM,\n",
    "    num_layers=config.NUM_LAYERS,\n",
    "    num_classes=config.NUM_CLASSES,\n",
    "    dropout=config.DROPOUT,\n",
    "    pretrained_embeddings=embedding_matrix,\n",
    "    word_dropout=config.WORD_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}  (trainable: {trainable_params:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53aa5244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:53.784105Z",
     "iopub.status.busy": "2026-02-06T20:10:53.783797Z",
     "iopub.status.idle": "2026-02-06T20:10:53.792232Z",
     "shell.execute_reply": "2026-02-06T20:10:53.791608Z",
     "shell.execute_reply.started": "2026-02-06T20:10:53.784076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([1.1434, 0.8937, 0.9936], device='cuda:0')\n",
      "Optimizer and scheduler initialized\n"
     ]
    }
   ],
   "source": [
    "# Class weights for imbalanced data\n",
    "class_weights = compute_class_weights(train_df['label'].values).to(device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE,\n",
    "                        weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# ReduceLROnPlateau — only reduces LR when val loss stops improving\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    ")\n",
    "\n",
    "print(\"Optimizer and scheduler initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dce27",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8dbd8580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:10:53.793312Z",
     "iopub.status.busy": "2026-02-06T20:10:53.793055Z",
     "iopub.status.idle": "2026-02-06T20:11:18.675353Z",
     "shell.execute_reply": "2026-02-06T20:11:18.674492Z",
     "shell.execute_reply.started": "2026-02-06T20:10:53.793292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training...\n",
      "================================================================================\n",
      "Epoch 1/9\n",
      "  Train — Loss: 0.8731  Acc: 0.5790  F1: 0.5751\n",
      "  Val   — Loss: 0.7593  Acc: 0.6617  F1: 0.6612\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2/9\n",
      "  Train — Loss: 0.7631  Acc: 0.6561  F1: 0.6551\n",
      "  Val   — Loss: 0.7329  Acc: 0.6774  F1: 0.6780\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3/9\n",
      "  Train — Loss: 0.7382  Acc: 0.6708  F1: 0.6702\n",
      "  Val   — Loss: 0.7223  Acc: 0.6824  F1: 0.6826\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4/9\n",
      "  Train — Loss: 0.7267  Acc: 0.6796  F1: 0.6796\n",
      "  Val   — Loss: 0.7109  Acc: 0.6770  F1: 0.6738\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5/9\n",
      "  Train — Loss: 0.7125  Acc: 0.6833  F1: 0.6835\n",
      "  Val   — Loss: 0.7064  Acc: 0.6895  F1: 0.6896\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6/9\n",
      "  Train — Loss: 0.6993  Acc: 0.6904  F1: 0.6906\n",
      "  Val   — Loss: 0.7074  Acc: 0.6778  F1: 0.6721\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7/9\n",
      "  Train — Loss: 0.6846  Acc: 0.6978  F1: 0.6981\n",
      "  Val   — Loss: 0.6925  Acc: 0.6972  F1: 0.6991\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8/9\n",
      "  Train — Loss: 0.6768  Acc: 0.7027  F1: 0.7031\n",
      "  Val   — Loss: 0.6968  Acc: 0.6972  F1: 0.6995\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9/9\n",
      "  Train — Loss: 0.6630  Acc: 0.7078  F1: 0.7085\n",
      "  Val   — Loss: 0.6961  Acc: 0.7024  F1: 0.7047\n",
      "  LR: 0.001000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"\\n Training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_val_f1 = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [],\n",
    "           'train_f1': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, config.GRADIENT_CLIP\n",
    "    )\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "    print(f\"  Train — Loss: {train_loss:.4f}  Acc: {train_acc:.4f}  F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   — Loss: {val_loss:.4f}  Acc: {val_acc:.4f}  F1: {val_f1:.4f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b85ce3",
   "metadata": {},
   "source": [
    "## 13. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d5846f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T20:11:18.677232Z",
     "iopub.status.busy": "2026-02-06T20:11:18.676891Z",
     "iopub.status.idle": "2026-02-06T20:11:18.969587Z",
     "shell.execute_reply": "2026-02-06T20:11:18.968849Z",
     "shell.execute_reply.started": "2026-02-06T20:11:18.677203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL TEST RESULTS\n",
      "================================================================================\n",
      "Test Loss: 0.7059\n",
      "Test Accuracy: 0.6905\n",
      "Test F1 Score: 0.6926\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6807    0.7089    0.6946      1546\n",
      "     Neutral     0.6134    0.6601    0.6359      1930\n",
      "    Positive     0.8065    0.7081    0.7541      1730\n",
      "\n",
      "    accuracy                         0.6905      5206\n",
      "   macro avg     0.7002    0.6924    0.6948      5206\n",
      "weighted avg     0.6975    0.6905    0.6926      5206\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1096  397   53]\n",
      " [ 415 1274  241]\n",
      " [  99  406 1225]]\n",
      "\n",
      "Per-class Accuracy:\n",
      "  Negative: 0.7089\n",
      "  Neutral: 0.6601\n",
      "  Positive: 0.7081\n"
     ]
    }
   ],
   "source": [
    "# Load best model & evaluate on test set\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds,\n",
    "                            target_names=['Negative', 'Neutral', 'Positive'],\n",
    "                            digits=4))\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "print(f\"\\nPer-class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_acc):\n",
    "    label = ['Negative', 'Neutral', 'Positive'][i]\n",
    "    print(f\"  {label}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a92b0",
   "metadata": {},
   "source": [
    "## 14. Key Observations and Analysis\n",
    "\n",
    "### Training Dynamics:\n",
    "1. **Consistent Improvement**: The model showed steady improvement from epoch 1 to epoch 9\n",
    "   - Training F1: 0.5751 → 0.7085 (+13.34%)\n",
    "   - Validation F1: 0.6612 → 0.7047 (+4.35%)\n",
    "\n",
    "2. **Overfitting is Well Controlled**: \n",
    "   - Gap between train and val F1 at epoch 9: 0.7085 - 0.7047 = 0.0038 (only 0.38%)\n",
    "   - This confirms that the regularization techniques (dropout 0.4, word dropout 0.05, weight decay 3e-3) are effective\n",
    "\n",
    "### Model Performance:\n",
    "3. **Strong Test Performance**:\n",
    "   - Test F1: **0.6926** (very close to validation F1 of 0.7047)\n",
    "   - Test Accuracy: **69.05%**\n",
    "   - This indicates good generalization with no indication of overfitting to validation set\n",
    "\n",
    "4. **Class-wise Performance**:\n",
    "   - **Negative (70.89% accuracy)**: Best performing class - likely has clearer linguistic patterns\n",
    "   - **Positive (70.81% accuracy)**: Robust performance - often confused with Neutral but less than Negative\n",
    "   - **Neutral (66.01% accuracy)**: Most challenging - often confused with both Negative and Positive\n",
    "\n",
    "5. **Confusion Matrix Insights**:\n",
    "   - Neutral tweets are frequently misclassified:\n",
    "   - This is expected as neutral sentiment is inherently ambiguous\n",
    "     \n",
    "### Technical Highlights:\n",
    "6. **Word2Vec Coverage**: 99.99% (14998/15000 words found)\n",
    "   - Excellent coverage indicates high-quality embeddings\n",
    "   - Frozen embeddings work well when coverage is this high\n",
    "\n",
    "7. **Model Efficiency**:\n",
    "   - Total parameters: 2,112,580\n",
    "   - Trainable parameters: 192,580 (only 9.1% of total)\n",
    "   - Most parameters are in frozen embeddings, preventing overfitting\n",
    "\n",
    "8. **Class Imbalance Handling**:\n",
    "   - Class weights successfully balanced the training:\n",
    "     - Negative (9105 samples): weight 1.14\n",
    "     - Neutral (11649 samples): weight 0.89\n",
    "     - Positive (10478 samples): weight 0.99\n",
    "\n",
    "### Conclusion:\n",
    "- The Bidirectional LSTM with attention and frozen Word2Vec embeddings achieved strong performance on the multiclass sentiment analysis task.\n",
    "- Regularization techniques effectively controlled overfitting, allowing the model to generalize well to the test set.\n",
    "- Future work could explore more advanced architectures (e.g., transformers) to further boost performance, especially on the challenging Neutral class."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
